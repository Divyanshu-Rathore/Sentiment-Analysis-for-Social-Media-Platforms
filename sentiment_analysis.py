# -*- coding: utf-8 -*-
"""Sentiment Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hbL_neVtz0p64gP8t5cGPfeYA2zTCd8r
"""

from textblob import TextBlob
from google.colab import drive

import re
import nltk
import string
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

drive.mount('/content/drive')

train = pd.read_csv('/content/drive/MyDrive/Placement/Twitter Sentiment Data/TSA_train.csv')
test = pd.read_csv('/content/drive/MyDrive/Placement/Twitter Sentiment Data/TSA_test.csv')
train

"""**Data Inspection**"""

# Check the length of the tweet

ln_train = train['tweet'].str.len()
ln_test = test['tweet'].str.len()

plt.hist(ln_train, bins=20,label='train tweets')
plt.hist(ln_test,bins=20,label='test tweets')
plt.legend()
plt.show()

""" **Data Cleaning** """

data = train.append(test, ignore_index=True)
import re

# Make functions to remove unwanted patterns

def remove_pattern(input_txt, pattern):
  r = re.findall(pattern,input_txt)
  for i in r:
    input_txt = re.sub(i,'', input_txt)
  return input_txt


# Remove twitter handles using above  function

data['tidy_tweet'] = np.vectorize(remove_pattern)(data['tweet'], "@[\w]")

# Remove Punctuatino, number  special character
data['tidy_tweet'] = data['tidy_tweet'].str.replace("[^a-zA-Z#]", " ")

# Remove Short Words
data['tidy_tweet'] = data['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))

# Normalization

tokenize_tweet = data['tidy_tweet'].apply(lambda x: x.split())

from nltk.stem.porter import *
stemmer = PorterStemmer()
tokenize_tweet = tokenize_tweet.apply(lambda x: [stemmer.stem(i) for i in x])


for i in range(len(tokenize_tweet)):
  tokenize_tweet[i] = ' '.join(tokenize_tweet[i])

data['tidy_tweet'] = tokenize_tweet

data

"""**Story Generation & Visualization from text data**"""

# USe word cloud

all_word = ' '.join([text for text in data['tidy_tweet']])  # Make a simple paragraph text by joining all the tweets
from wordcloud import WordCloud  # import word cloud library
wordcloud = WordCloud(width=800, height=500, random_state= 21, max_font_size=110).generate(all_word) # first define size of clouds like width and height then generate the wordcloud

# Plot the wordcloud
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Plot wordcloud for Non-racist

normal_word = ' '.join([text for text in data['tidy_tweet'][data['label']==0]])  # Make a simple paragraph text by joining all the tweets
from wordcloud import WordCloud  # import word cloud library
wordcloud = WordCloud(width=800, height=500, random_state= 21, max_font_size=110).generate(normal_word) # first define size of clouds like width and height then generate the wordcloud

# Plot the wordcloud
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Plot wordcloud for Racist/Sexist


negative_word = ' '.join([text for text in data['tidy_tweet'][data['label']==1]])  # Make a simple paragraph text by joining all the tweets
from wordcloud import WordCloud  # import word cloud library
wordcloud = WordCloud(width=800, height=500, random_state= 21, max_font_size=110).generate(negative_word) # first define size of clouds like width and height then generate the wordcloud

# Plot the wordcloud
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

# Plot Impact of hastags on Tweets


# Make function to extract hashtags
def hash_tag(x):
  hashtags = [ ]
  for i in x:
    ht = re.findall(r"#(\w+)", i)
    hashtags.append(ht)

  return hashtags




# Extract Hashtags from Non-racist/sexist tweet

ht_normal = hash_tag(data['tidy_tweet'][data['label']==0])

# Extract Hashtags from Non-racist/sexist tweet

ht_negative = hash_tag(data['tidy_tweet'][data['label']==1])



# Plot Hashtags from Non-racist/sexist tweet

ht_normal  = sum(ht_normal ,[])
a = nltk.FreqDist(ht_normal)
d = pd.DataFrame({ 'Hashtags': list(a.keys()), 'Count': list(a.values()) })

d = d.nlargest(columns="Count", n=20)
plt.figure(figsize= (16,5))
plt.title('For Non-Racist/Sexist')
ax  = sns.barplot(data =d, x="Hashtags", y="Count")
ax.set(ylabel='Count')
plt.show()




# Plot Hashtags from racist/sexist tweet

ht_negative = sum(ht_negative, [])

b = nltk.FreqDist(ht_negative)
e = pd.DataFrame({ 'Hashtags': list(b.keys()), 'Count': list(b.values())})

e = e.nlargest(columns="Count", n=20)

plt.figure(figsize= (16,5))
plt.title('For Racist/Sexist')
ax  = sns.barplot(data =e, x="Hashtags", y="Count")
ax.set(ylabel='Count')
plt.show()

"""**Bag-of-Words Features**"""

# Bag-of-Wrods Technique

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.feature_extraction.text import CountVectorizer
import gensim

bow_vectorizer =  CountVectorizer(max_df =0.9, min_df=2, max_features = 1000, stop_words= 'english')

bow =  bow_vectorizer.fit_transform(data['tidy_tweet'])

"""**TF-IDF Features**"""

# Another

tfidf_vectorizer = TfidfVectorizer(max_df= 0.90, min_df =2,  max_features = 1000, stop_words= 'english')
tfidf =  tfidf_vectorizer.fit_transform(data['tidy_tweet'])

"""**Word2Vec **"""

# Train Word2Vec Model 
tokenized_tweet = data['tidy_tweet'].apply(lambda x: x.split()) 

model_w2v = gensim.models.Word2Vec( tokenized_tweet, size=200,  window=5, min_count=2, sg = 1, hs = 0,negative = 10, workers= 2, seed = 34)
model_w2v.train(tokenized_tweet, total_examples= len(data['tidy_tweet']),epochs=20)

# Below function make Vectors for each tweet
def word_vector(tokens, size):
  vec = np.zeros(size).reshape((1,size))
  count = 0
  for word in tokens:
    try:
      vec = vec + model_w2v[word].reshape((1,size))
      count = count + 1
    except KeyError:
      continue
  if count != 0:
    vec /= count
  return vec


wordvec_arrays = np.zeros((len(tokenized_tweet), 200))
for i in range(len(tokenized_tweet)):
  wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)
  wordvec_df = pd.DataFrame(wordvec_arrays) 

wordvec_df.shape

"""**X-Boost**"""

from sklearn.model_selection import train_test_split

train_bow = bow[:31962,:] 
test_bow= bow[31962:,:]

xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size = 0.3)

train_w2v = wordvec_df.iloc[:31962,:] 
test_w2v = wordvec_df.iloc[31962:,:]




xtrain_w2v = train_w2v.iloc[ytrain.index,:] 
xvalid_w2v = train_w2v.iloc[yvalid.index, :]

import xgboost as xgb

xgb_cl = xgb.XGBClassifier(max_depth = 6, n_estimators =1000, nthread =3)

xgb_cl.fit(xtrain_w2v,ytrain)
prediction = xgb_cl.predict(xvalid_w2v)

from sklearn.metrics import f1_score
f1_score(yvalid, prediction)

# Export Result
import pandas as pd

sample = pd.read_csv('/content/drive/MyDrive/Placement/Twitter Sentiment Data/TSA_sample.csv')

df = pd.DataFrame(prediction)

sample['label_1'] = df
sample['label_1'] = sample['label_1'].fillna(0)



sample.to_csv("data2.csv")

sample









